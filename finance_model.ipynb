{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c257bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import(\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    TrainingArguments, Trainer, DataCollatorWithPadding\n",
    ")\n",
    "import torch\n",
    "import gc\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87047838",
   "metadata": {},
   "source": [
    "Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5110832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Reference Code              Date Time  \\\n",
      "0          0WPVCY9    2025-05-28 14:32:57   \n",
      "1          0XRKVN9  2025-05-27 21:59:03.0   \n",
      "2          0XRHORM  2025-05-27 20:51:19.0   \n",
      "3          0XR8ZO5  2025-05-27 19:10:16.0   \n",
      "4          0XQDXTQ  2025-05-27 11:20:44.0   \n",
      "..             ...                    ...   \n",
      "133        0VCPH57  2025-04-03 16:04:08.0   \n",
      "134        0VCPDU2  2025-04-03 16:02:41.0   \n",
      "135        0VC723Q  2025-04-03 10:56:21.0   \n",
      "136        0VC723Q  2025-04-03 10:56:21.0   \n",
      "137        0V9K8PE  2025-04-01 15:58:06.0   \n",
      "\n",
      "                                   Description    Dr.    Cr.    Status  \\\n",
      "0        Paid for Dipesh Hair Cutting & Saloon  150.0    0.0  COMPLETE   \n",
      "1             Fund Transferred by Biplov Malla    0.0  200.0  COMPLETE   \n",
      "2      Fund Transferred to Bel Prasad Shrestha  400.0    0.0  COMPLETE   \n",
      "3                  Paid for NANDINI FOOD COURT  370.0    0.0  COMPLETE   \n",
      "4                     Paid for Shahi Suppliers   80.0    0.0  COMPLETE   \n",
      "..                                         ...    ...    ...       ...   \n",
      "133  Fund Transferred to Ashraya Jung Sijapati  500.0    0.0  COMPLETE   \n",
      "134                   Paid for HAMRO ADDA CAFE  100.0    0.0  COMPLETE   \n",
      "135                      Bank transfer charges   10.0    0.0  COMPLETE   \n",
      "136      Money transferred to SANIMA BANK LTD.  100.0    0.0  COMPLETE   \n",
      "137          Paid for Gopi    Krishna    Store  100.0    0.0  COMPLETE   \n",
      "\n",
      "     Balance (NPR) Channel              Category  \n",
      "0           173.21     App         Personal Care  \n",
      "1           323.21     App                Income  \n",
      "2           123.21     App     Banking & Finance  \n",
      "3           523.21     App         Dining & Food  \n",
      "4           893.21     App  Groceries & Shopping  \n",
      "..             ...     ...                   ...  \n",
      "133          11.36     App     Banking & Finance  \n",
      "134         511.36     App         Dining & Food  \n",
      "135         611.36     App     Banking & Finance  \n",
      "136         621.36     App     Banking & Finance  \n",
      "137         721.36     App  Groceries & Shopping  \n",
      "\n",
      "[138 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel('Pralov.xls', skiprows=8)\n",
    "df = df.dropna()\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2756e5c7",
   "metadata": {},
   "source": [
    "## Basic Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "038d7623",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\swoye\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\swoye\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\swoye\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\swoye\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\swoye\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44bcdd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "stopwords = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51acd1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   Description  \\\n",
      "0        Paid for Dipesh Hair Cutting & Saloon   \n",
      "1             Fund Transferred by Biplov Malla   \n",
      "2      Fund Transferred to Bel Prasad Shrestha   \n",
      "3                  Paid for NANDINI FOOD COURT   \n",
      "4                     Paid for Shahi Suppliers   \n",
      "..                                         ...   \n",
      "133  Fund Transferred to Ashraya Jung Sijapati   \n",
      "134                   Paid for HAMRO ADDA CAFE   \n",
      "135                      Bank transfer charges   \n",
      "136      Money transferred to SANIMA BANK LTD.   \n",
      "137          Paid for Gopi    Krishna    Store   \n",
      "\n",
      "                 processedDescription  \n",
      "0            paid hair cutting saloon  \n",
      "1              fund transferred malla  \n",
      "2    fund transferred prasad shrestha  \n",
      "3                     paid food court  \n",
      "4                 paid shahi supplier  \n",
      "..                                ...  \n",
      "133    fund transferred jung sijapati  \n",
      "134              paid hamro adda cafe  \n",
      "135              bank transfer charge  \n",
      "136        money transferred bank ltd  \n",
      "137           paid gopi krishna store  \n",
      "\n",
      "[138 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "def textPreprocessing(text):\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = text.lower()  \n",
    "    text = word_tokenize(text)\n",
    "    return text\n",
    "\n",
    "def stopwordRemoval(text):\n",
    "    text = [word for word in text if word not in stopwords]\n",
    "    return text\n",
    "\n",
    "def lemmatization(text):\n",
    "    text = [lemmatizer.lemmatize(word) for word in text]\n",
    "    return text \n",
    "\n",
    "def posTagging(text):\n",
    "    text = nltk.pos_tag(text)\n",
    "    text = [word for word, tag in text if tag.startswith('N') or tag.startswith('V')]   \n",
    "    return text\n",
    "\n",
    "def cleanTextPipeline(text):\n",
    "    text = textPreprocessing(text)\n",
    "    text = stopwordRemoval(text)\n",
    "    text = lemmatization(text)\n",
    "    text = posTagging(text)\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "df['processedDescription'] = df[\"Description\"].apply(lambda x : cleanTextPipeline(x))\n",
    "\n",
    "print(df[['Description', 'processedDescription']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d59c4e",
   "metadata": {},
   "source": [
    "Spacy Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2faba61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0             pay dipesh hair cut saloon\n",
      "1                          fund transfer\n",
      "2      fund transfer bel prasad shrestha\n",
      "3                         pay food court\n",
      "4                           pay supplier\n",
      "                     ...                \n",
      "133                        fund transfer\n",
      "134                                  pay\n",
      "135                 bank transfer charge\n",
      "136                       money transfer\n",
      "137                       pay gopi store\n",
      "Name: Description, Length: 138, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def cleanTextPipelineSpacy(text):\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = text.lower()\n",
    "\n",
    "    doc = nlp(text) \n",
    "\n",
    "    tokens = [token for token in doc if not token.is_stop and not token.is_punct]\n",
    "\n",
    "    lemmas = [token.lemma_ for token in tokens]\n",
    "\n",
    "    filtered_tokens = [lemma for lemma, token in zip(lemmas, tokens) if token.pos_ in ['NOUN', 'VERB']]\n",
    "\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "new_try = df['Description'].apply(lambda x: cleanTextPipelineSpacy(x))\n",
    "\n",
    "print(new_try)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4687469b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Reference Code', 'Date Time', 'Description', 'Dr.', 'Cr.', 'Status',\n",
      "       'Balance (NPR)', 'Channel', 'Category', 'processedDescription'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73225616",
   "metadata": {},
   "source": [
    "## Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1a05eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransactionDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        # Convert texts and labels to lists to ensure indexing works properly\n",
    "        self.texts = texts.reset_index(drop=True)\n",
    "        self.labels = labels.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Verify data integrity\n",
    "        assert len(self.texts) == len(self.labels), \"Texts and labels must have the same length\"\n",
    "        print(f\"Dataset created with {len(self.texts)} examples\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.texts):\n",
    "            raise IndexError(f\"Index {idx} out of bounds for dataset of size {len(self.texts)}\")\n",
    "        \n",
    "        try:\n",
    "            text = str(self.texts.iloc[idx])\n",
    "            label = self.labels.iloc[idx]\n",
    "            \n",
    "            encoding = self.tokenizer(\n",
    "                text,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=self.max_length,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "        \n",
    "            return {\n",
    "                'input_ids': encoding['input_ids'].flatten(),\n",
    "                'attention_mask': encoding['attention_mask'].flatten(),    \n",
    "                'labels': torch.tensor(label, dtype=torch.long)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing item at index {idx}\")\n",
    "            print(f\"Text: {self.texts.iloc[idx] if idx < len(self.texts) else 'Index out of bounds'}\")\n",
    "            print(f\"Label: {self.labels.iloc[idx] if idx < len(self.labels) else 'Index out of bounds'}\")\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b9703f",
   "metadata": {},
   "source": [
    "Prepare transaction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8e85bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_bert_data(df):\n",
    "    df['combined_text'] = df['processedDescription'] + ' ' + df['Dr.'].astype(str) + ' ' + df['Cr.'].astype(str) + ' ' + df['Balance (NPR)'].astype(str) + ' ' + df['Channel'] + ' ' + df['Status']\n",
    "    unique_labels = df['Category'].unique()\n",
    "    label_to_id = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "    id_to_label = {idx: label for label, idx in label_to_id.items()}\n",
    "\n",
    "    df['label_id'] = df['Category'].map(label_to_id)\n",
    "\n",
    "    print(f\"Found {len(unique_labels)} unique labels.\")\n",
    "    print(\"Label mappings:\")\n",
    "    for label, idx in label_to_id.items():\n",
    "        print(f\" {idx}: {label}\")\n",
    "    \n",
    "    print(\"\\nLabel ID range in dataset:\")\n",
    "    print(f\"Min: {df['label_id'].min()}, Max: {df['label_id'].max()}\")\n",
    "    \n",
    "    return df, label_to_id, id_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d149963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bert_model(df, model_name = \"distilbert-base-uncased\", test_size = 0.2, epochs = 3, batch_size = 16):\n",
    "    df, label_to_id, id_to_label = prepare_bert_data(df)\n",
    "    \n",
    "    X = df['combined_text']\n",
    "    Y = df['label_id']\n",
    "    \n",
    "    # Print dataset statistics\n",
    "    print(f\"Total dataset size: {len(df)}\")\n",
    "    print(f\"Number of unique labels: {len(label_to_id)}\")\n",
    "    print(\"Label distribution:\")\n",
    "    for label, count in df['Category'].value_counts().items():\n",
    "        print(f\" - {label}: {count}\")\n",
    "    \n",
    "    train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=test_size, random_state=42)   \n",
    "    \n",
    "    print(f\"\\nTrain set size: {len(train_X)}\")\n",
    "    print(f\"Test set size: {len(test_X)}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(label_to_id))\n",
    "    \n",
    "    train_dataset = TransactionDataset(train_X, train_Y, tokenizer)\n",
    "    test_dataset = TransactionDataset(test_X, test_Y, tokenizer)\n",
    "    \n",
    "    # Verify datasets\n",
    "    print(\"\\nVerifying datasets:\")\n",
    "    print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "    print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./transaction_classifier',\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        warmup_steps=100,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=10,\n",
    "        eval_steps=50,               \n",
    "        do_eval=True,                 \n",
    "        save_steps=50,                \n",
    "        save_total_limit=2,  \n",
    "        dataloader_num_workers=0,\n",
    "        dataloader_pin_memory=True,          \n",
    "    )\n",
    "    \n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    print(f\"Current device: {torch.cuda.current_device() if torch.cuda.is_available() else 'CPU'}\")\n",
    "\n",
    "    print(f\"Number of labels in mapping: {len(label_to_id)}\")\n",
    "    print(f\"Model output dimension: {model.config.num_labels}\")\n",
    "    \n",
    "    print(\"Starting the BERT model training...\")\n",
    "    print(f\"This will take approximately {epochs * len(train_dataset) // batch_size} minutes\")\n",
    "    trainer.train()\n",
    "    \n",
    "    print(\"Evaluating the model...\")\n",
    "    predictions = trainer.predict(test_dataset)\n",
    "    pred_labels = np.argmax(predictions.predictions, axis=1)\n",
    "    \n",
    "    accuracy = accuracy_score(test_Y, pred_labels)\n",
    "    \n",
    "    print(f\"\\nValidation Accuracy: {accuracy:.4f}\")\n",
    "    print(\"\\nDetailed Classification Report:\")\n",
    "    print(classification_report(\n",
    "        test_Y.values, pred_labels, target_names=[id_to_label[i] for i in range(len(id_to_label))]\n",
    "    ))\n",
    "    \n",
    "    return trainer, tokenizer, (label_to_id, id_to_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c57cf5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_bert(trainer, tokenizer, label_mappings, texts, return_probabilities = False):\n",
    "    label_to_id, id_to_label = label_mappings\n",
    "    model = trainer.model\n",
    "    num_labels = len(id_to_label)\n",
    "\n",
    "    print(f\"Making predictions for {len(texts)} transactions...\")\n",
    "\n",
    "    encodings = tokenizer(\n",
    "        texts, \n",
    "        truncation=True, \n",
    "        padding=True, \n",
    "        return_tensors='pt', \n",
    "        max_length=128  \n",
    "    )\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encodings)\n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "    pred_labels = torch.argmax(predictions, dim=-1).numpy()\n",
    "    \n",
    "    # Add safety check for label indices\n",
    "    final_predictions = []\n",
    "    for label in pred_labels:\n",
    "        if label >= num_labels:\n",
    "            print(f\"Warning: Invalid label index {label}, defaulting to 0\")\n",
    "            final_predictions.append(id_to_label[0])\n",
    "        else:\n",
    "            final_predictions.append(id_to_label[label])\n",
    "\n",
    "    if return_probabilities:\n",
    "        return final_predictions, predictions.numpy()\n",
    "    \n",
    "    return final_predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d4ff08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_usage():\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Transaction Classification with BERT\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    trainer, tokenizer, label_mappings = train_bert_model(df, epochs = 2, batch_size = 4)   \n",
    "\n",
    "    new_transactions = [\n",
    "        \"Paid for medical store\",\n",
    "        \"Uber ride to airport\", \n",
    "        \"Netflix subscription payment\",\n",
    "        \"Grocery shopping at supermarket\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 40)\n",
    "    print(\"TESTING PREDICTIONS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    predictions = predict_with_bert(trainer, tokenizer, label_mappings, new_transactions)\n",
    "    \n",
    "    for transaction, prediction in zip(new_transactions, predictions):\n",
    "        print(f\"Transaction: '{transaction}'\")\n",
    "        print(f\"Predicted Category: {prediction}\")\n",
    "        print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ccb8f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "879de447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.52.4\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
